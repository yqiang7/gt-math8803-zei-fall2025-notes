\chapter{Nov.~12 --- Root Systems}

\section{Abstract Root Systems}

\begin{definition}
  An \emph{abstract root system}
  $R \subseteq E \setminus \{0\}$
  (where $E$ is a Euclidean vector space,
  i.e. a real vector space with inner
  product $(\cdot, \cdot)$) such that
  \begin{enumerate}[(R1)]
    \item $R$ generates $E$ as a
      vector space.
    \item For all $\alpha, \beta \in \R$,
      $n_{\alpha, \beta} = 2(\alpha, \beta) /
      (\alpha, \alpha) \in \Z$.
    \item Define
      $s_{\alpha} : E \to E$ by
      $s_\alpha(\lambda) = \lambda - 2((\alpha, \lambda) / (\alpha, \alpha)) \alpha$.
      Then for all $\alpha, \beta \in R$,
      $s_\alpha(\beta) \in R$.
  \end{enumerate}
  The number $r = \dim E$ is called
  the \emph{rank} of $R$.
  \begin{enumerate}
    \item[(R4)] If additionally
      $\alpha, c \alpha$ being
      roots implies $c = \pm 1$, then
      $R$ is called a \emph{reduced} root
      system.
  \end{enumerate}
\end{definition}

\begin{remark}
  Note that (R2) and (R3)
  have geometric meaning: For (R3), if we
  define
  \[L_\alpha = \{\lambda \in E : (\alpha, \lambda) = 0\},\]
  then $s_\alpha$ is the reflection
  across this hyperplane, i.e.
  $s_\alpha(\lambda) = \lambda$ if
  $(\alpha, \lambda) = 0$ and
  $s(\alpha) = -\alpha$. For (R2),
  if $p_\alpha$ is the projection onto
  the line containing $\alpha$, then
  $p_\alpha(\beta) = (h_{\beta, \alpha} / 2) \alpha$.

  In this language, we can reformulate
  Theorem \ref{thm:structure-semisimple} as follows:
\end{remark}

\begin{theorem}[Structure of semisimple Lie algebras]
  Let $\g$ be a complex
  semisimple Lie algebra with
  root decomposition. Then the set of
  roots $R \subseteq \mathfrak{h}^* \setminus \{0\}$
  is a reduced root system.
\end{theorem}

\begin{definition}
  For $\alpha \in R$, define the
  \emph{coroot}
  $\alpha^\vee \in E$ by
  \[
    \langle \alpha^\vee, \lambda \rangle
    = \frac{2(\alpha, \lambda)}{(\alpha, \alpha)}.
  \]
\end{definition}

\begin{remark}
  For a semisimple Lie algebra, we have
  $\alpha^\vee = h_{\alpha} \in \mathfrak{h}$,
  and
  \[
    \langle \alpha^\vee, \alpha \rangle
    = 2, \quad
    n_{\alpha, \beta}
    = \langle \alpha, \beta^\vee \rangle,
    \quad
    s_\alpha(\lambda)
    = \lambda - \langle \lambda, \alpha^\vee \rangle \alpha.
  \]
\end{remark}

\begin{example}
  Let $e_i$ be the standard basis vectors
  of $\R^n$, so
  $(e_i, e_j = \delta_{ij})$, and let
  \[
    E =
    \left\{
      (\lambda_1, \ldots, \lambda_n) \in \R^n :
      \sum_{i=1}^n \lambda_i = 0
    \right\}.
  \]
  Define $R = \{e_i - e_j : 1 \le i, j \le n,\, i \ne j\}$.
  Then we claim that
  $R$ is a reduced root system. Note that
  if we let
  $\alpha = e_i - e_j$, then one can check
  that we have
  \[
    s_{e_i - e_j}(\dots, \lambda_i, \dots, \lambda_j, \dots)
    = (\dots, \lambda_j, \dots, \lambda_i, \dots).
  \]
  Also check the remaining properties as
  an exercise.
  The $R$ defined above is a root system for
  $\mathfrak{sl}(n, \C)$, and it is of rank
  $n - 1$. It is denoted as a root system
  of \emph{type $A_{n - 1}$}.
\end{example}

\begin{definition}
  Let $R_1 \subseteq E_1$, $R_2 \subseteq E_2$
  be two root systems. An
  isomorphism $\varphi : R_1 \to R_2$
  is a vector space isomorphism
  $E_1 \to E_2$ such that
  $\varphi(R_1) = R_2$ and
  \[
    n_{\varphi(\alpha), \varphi(\beta)}
    = n_{\alpha, \beta}, \quad
    \alpha, \beta \in R_1.
  \]
  Note that the last condition is
  automatically satisfied if $\varphi$
  preserves the inner product.
\end{definition}

\begin{definition}
  The \emph{Weyl group} $W$ of a
  root system is the subgroup of
  $\GL(E)$ generated by the
  reflections $\{s_\alpha : \alpha \in R\}$.
\end{definition}

\begin{lemma}
  We have the following:
  \begin{enumerate}
    \item The Weyl group
      is a finite subgroup of $O(E)$
      and the root system is invariant
      under $W$.
    \item $s_{w(\alpha)} = w s_\alpha w^{-1}$
      for any $w \in W$,
      $\alpha \in R$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  (1) Any reflection is an orthogonal
  transformation. We also have
  $w(R) = R$ for any $w \in W$, so
  $R$ finite implies that
  $W \subseteq \Aut(R)$ is finite.

  (2) On the hyperplane
  $w L_\alpha = L_{w(\alpha)}$, we have
  \[
    w s_\alpha w^{-1} w L_\alpha
    = L_{w(\alpha)}
  \]
  since $w s_\alpha w^{-1}$ acts as the
  identity on this hyperplane. Also,
  \[
    w s_\alpha w^{-1} w(\alpha)
    = w s_\alpha(\alpha)
    = -w(\alpha),
  \]
  so we get that
  $w s_\alpha w^{-1} = s_{w(\alpha)}$.
\end{proof}

\begin{example}
  Let $R$ be a root system of
  type $A_{n - 1}$. Then
  $W = S_n$. For
  root systems of type $A_1$, we have
  $W = \Z / 2\Z = S_2 = \{1, s\}$,
  where $s : \alpha \mapsto -\alpha$.
\end{example}

\section{Reduced Root Systems of Rank 2}

\begin{prop}
  Consider roots $\alpha, \beta$ with
  $\alpha \ne c \beta$. Assume
  $|\alpha| \ge |\beta|$, where
  $|\alpha| = \sqrt{(\alpha, \alpha)}$.
  Let $\varphi$ be the angle between
  $\alpha$ and $\beta$. Then we
  can only have the following possibilities:
  \begin{enumerate}
    \item $\varphi = \pi / 2$ (i.e.
      $\alpha, \beta$ are orthogonal)
      with $n_{\alpha, \beta} = n_{\beta, \alpha} = 0$.
    \item[2a.] $\varphi = 2\pi / 3$ with
      $|\alpha| = |\beta|$
      and $n_{\alpha, \beta} = n_{\beta, \alpha} = -1$.
    \item[2b.] $\varphi = \pi / 3$ with
      $|\alpha| = |\beta|$
      and $n_{\alpha, \beta} = n_{\beta, \alpha} = 1$.
    \item[3a.] $\varphi = 3\pi / 4$ with
      $|\alpha| = \sqrt{2} |\beta|$
      and $n_{\alpha, \beta} = -2$,
      $n_{\beta, \alpha} = -1$.
    \item[3b.] $\varphi = \pi / 4$ with
      $|\alpha| = \sqrt{2} |\beta|$
      and $n_{\alpha, \beta} = 2$,
      $n_{\beta, \alpha} = 1$.
    \item [4a.]
      $\varphi = 5\pi / 6$ with
      $|\alpha| = \sqrt{3} |\beta|$
      and $n_{\alpha, \beta} = -3$,
      $n_{\beta, \alpha} = -1$.
    \item[4b.] $\varphi = \pi / 6$ with
      $|\alpha| = \sqrt{3} |\beta|$
      and $n_{\alpha, \beta} = 3$,
      $n_{\beta, \alpha} = 1$.
  \end{enumerate}
\end{prop}

\begin{proof}
  We can write $(\alpha, \beta) = |\alpha| |\beta| \cos \varphi$,
  then
  $n_{\alpha, \beta} = 2 (|\alpha| / |\beta|) \cos \varphi$, so
  \[
    n_{\alpha, \beta}
    n_{\beta, \alpha}
    = 4 \cos^2 \varphi \in \Z.
  \]
  Thus we must have
  $n_{\alpha, \beta} n_{\beta, \alpha} = 0, 1, 2, 3$.
  The length statement follows from
  $n_{\alpha, \beta} / n_{\beta, \alpha} = |\alpha|^2 / |\beta|^2$.
\end{proof}

\begin{example}
  Let $A_1 \cup A_1$ be two perpendicular
  line segments (one vertical and one
  horizontal), $A_2$ be the diagonals of
  a regular hexagon, $B_2$ be
  $A_1 \cup A_1$ plus the diagonals of
  a square, and $G_2$ be two copies
  of $A_2$ with scales
  $1$ and $\sqrt{3}$ and angles
  $\pi / 6$ between all segments.
  See Kirillov Section 7.4.
\end{example}

\begin{theorem}
  We have the following:
  \begin{enumerate}
    \item Consider
      $A_1 \cup A_1, B_2, A_2, G_2$
      as sets of vectors in $\R^2$. Then
      each of them is a rank $2$ root system.
    \item Any rank $2$ reduced root system
      is isomorphic to one of these.
  \end{enumerate}
\end{theorem}

\begin{proof}
  Left as an exercise.
\end{proof}

\begin{lemma}\label{lem:sum-roots}
  Let $\alpha, \beta \in R$ such that
  $\alpha \ne c \beta$ and
  $(\alpha, \beta) < 0$. Then
  $\alpha + \beta \in R$.
\end{lemma}

\begin{proof}
  It is enough to check this for rank
  $2$, which can be done directly using the pictures.
\end{proof}

\section{Positive and Simple Roots}

\begin{remark}
  Let $t \in E$ such that
  $(t, \alpha) \ne 0$ for all $\alpha \in R$
  (such elements are called \emph{regular}).
  Then
  \[
    R = R_+ \sqcup R_-,
  \]
  where $R_+ = \{\alpha \in R : (\alpha, t) > 0\}$
  is the set of \emph{positive roots}
  and $R_- = \{\alpha \in R : (\alpha, t) < 0\}$
  is the set of \emph{negative roots}
  (note that $R_+, R_-$ depend on the
  choice of $t$). This decomposition
  is called a \emph{polarization} of $R$.
  Assume from now on that we have a
  fixed polarization of $R$.
\end{remark}

\begin{definition}
  A root $\alpha \in R_+$ is called
  \emph{simple} if it cannot be written
  as a sum of two positive roots. Denote
  the set of simple roots by $\Pi$.
\end{definition}

\begin{lemma}
  Any positive root can be written as a
  sum of simple roots.
\end{lemma}

\begin{proof}
  If a root $\alpha$ is not simple, then
  we can write $\alpha = \alpha' + \alpha''$
  with
  \[
    (\alpha', t) < (\alpha, t)
    \quad\text{and}\quad
    (\alpha'', t) < (\alpha, t).
  \]
  If $\alpha'$ or $\alpha''$ is not simple,
  then we can iterate this process.
  Since $(\alpha, t)$ can only take
  finitely many values,
  this process will eventually terminate.
\end{proof}

\begin{example}
  Consider the root system $A_2$
  with $\alpha_1$ pointing right and
  $\alpha_2$ pointing $60^\circ$
  counterclockwise from $\alpha_1$.
  The $\alpha_1 + \alpha_2$ lies between
  them, and we can choose a regular element
  $t$ to point between
  $\alpha_1 + \alpha_2$ and $\alpha_1$,
  closer to $\alpha_1 + \alpha_2$.
  Then $\alpha_1, \alpha_2$ are simple
  roots.
\end{example}

\begin{lemma}
  If $\alpha, \beta \in R_+$ are simple and
  $\alpha \ne \beta$, then
  $(\alpha, \beta) \le 0$.
\end{lemma}

\begin{proof}
  Assume $(\alpha, \beta) > 0$. Apply
  Lemma \ref{lem:sum-roots} to
  $-\alpha, \beta$ to get that
  $\beta' = \beta - \alpha \in R$.
  If $\beta' \in R_-$, then
  $-\beta' \in R_+$, so
  $\alpha = -\beta' + \beta$ cannot
  be simple. Contradiction.
\end{proof}

\begin{theorem}
  Let $R = R_+ \cup R_- \subseteq E$ be a
  root system. Then the simple roots
  form a basis of $E$.
\end{theorem}

\begin{proof}
  We have already seen that
  $\mathrm{span}(\Pi) = R$. The result
  then follows from:
  \begin{quote}
    \vspace{-2em}
    \begin{lemma}
      If $v_1, \dots, v_k$ is a collection
      of nonzero vectors in a Euclidean
      space $E$ such that for
      $(v_i, v_j) \le 0$ for
      $i \ne j$ and
      $(v_i, t) > 0$ for some $t$,
      then $v_1, \dots, v_k$ are
      linearly independent.
    \end{lemma}
  \end{quote}
  This is an exercise in linear algebra.
\end{proof}

\begin{corollary}
  For any $\alpha \in R_+$, we can write it
  as a linear combination of
  simple roots such that
  \[
    \alpha = \sum_{i = 1}^r n_i \alpha_i, \quad
    n_i \ge 0,
  \]
  where $r$ is the rank of the root system.
  A similar statement holds for
  $R_-$ with $n_i \le 0$.
\end{corollary}

\begin{definition}
  For a root $\alpha = \sum n_i \alpha_i \in R_+$, define
  its \emph{height} to be
  $\height(\alpha) = \sum_{i = 1}^r n_i$.
\end{definition}

\begin{remark}
  We have $\height(\alpha_i) = 1$ for a
  simple root $\alpha_i$.
\end{remark}

\begin{example}
  Let $R$ be of $A_{n - 1}$ type. Then
  \[
    R_+ = \{e_i - e_j : i < j\}
    \quad \text{and} \quad
    R_- = \{e_i - e_j : i > j\}.
  \]
  The simple roots are
  $\alpha_1 = e_1 - e_2$,
  $\alpha_2 = e_2 - e_3$, \dots,
  $\alpha_{n - 1} = e_{n - 1} - e_n$.
\end{example}
